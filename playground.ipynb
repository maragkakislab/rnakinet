{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f759a835-3c01-4bf2-b1d7-b2124648b5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_2022', 'neg_2022', 'pos_2020', 'neg_2020', 'remdesivir_33', 'ac4C_10', '2-OmeATP_10', 'remdesivir_0', 's4U_33', 's4U_5', 'm5C_10', 'm5C_5', '2-OmeATP_0', 's4U_0', 'm6A_33', 'ac4C_33', '2-OmeATP_5', 'remdesivir_5', 's4U_10', 'm5C_0', 'm6A_5', 'm6A_0', 'ac4C_0', 'm6A_10'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datamap import experiment_files\n",
    "experiment_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f6474-785e-4598-b8e5-195ebb3b005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "import torch\n",
    "target = torch.tensor([1, 1, 1,0, 0,0,0])\n",
    "preds = torch.tensor([0, 1,0,1,0, 0,0])\n",
    "confmat = ConfusionMatrix(num_classes=2, normalize='true')\n",
    "confmat(preds, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0944f8-af21-4678-b952-51913c8e8c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datamap import experiment_files\n",
    "len(list(experiment_files['pos_2022_fast'][0].iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cdb485-4ab6-463d-a663-bbd5ce1ed571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid files indicies\n",
      "[14, 34, 36, 41, 46, 87, 116, 189, 193, 254]\n",
      "[14, 41, 68, 71, 87, 165, 189, 203, 228, 254]\n",
      "train files indicies\n",
      "[54, 68, 71, 165, 203, 228, 229]\n",
      "[34, 36, 46, 54, 116, 193, 229]\n"
     ]
    }
   ],
   "source": [
    "from dataloading2 import nanopore_datamodule\n",
    "dm = nanopore_datamodule(pos_files = 'pos_2022_fast', neg_files='pos_2022_fast', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1477dc2-ae4a-4df7-81cb-80c83de6a856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/RNAmodEnv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jovyan/my-conda-envs/RNAmodEnv/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import RNN\n",
    "import torchmetrics\n",
    "from resnet1d.resnet1d import ResNet1D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f62e90-e1c7-4037-b0c0-29ca6bcaa35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "BertModel(BertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a11fc3e-7a15-464f-9337-fb1567539022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using custom-modified transformer architecture, not the original source code\n",
      "SKIPPING EMBEDDING, d_iput and d_model must be equal\n",
      "torch.Size([32, 1])\n",
      "Transformer(\n",
      "  (layers_encoding): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (layers_decoding): ModuleList(\n",
      "    (0): Decoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_encoderDecoderAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Decoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_encoderDecoderAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): Decoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_encoderDecoderAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (3): Decoder(\n",
      "      (_selfAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_encoderDecoderAttention): MultiHeadAttention(\n",
      "        (_W_q): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_k): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_v): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (_W_o): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (_feedForward): PositionwiseFeedForward(\n",
      "        (_linear1): Linear(in_features=1, out_features=2048, bias=True)\n",
      "        (_linear2): Linear(in_features=2048, out_features=1, bias=True)\n",
      "      )\n",
      "      (_layerNorm1): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm2): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_layerNorm3): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "      (_dopout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (_embedding): Identity()\n",
      "  (_linear): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (_mylinear): Linear(in_features=1000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from tst import Transformer\n",
    "from transformer.src.benchmark import LSTM, BiGRU, ConvGru, FullyConv, FFN\n",
    "import torch\n",
    "# LSTM(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "# BiGRU(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "# ConvGru(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "# FullyConv(input_dim=1, hidden_dim=64, output_dim=1)\n",
    "# FFN(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "\n",
    "#TODO chunk=none??\n",
    "#TODO attention size\n",
    "leng = 1000\n",
    "model = Transformer(d_input=1, d_model=1, d_output=1,input_length=1000, q=8,v=8,h=8, N=4, attention_size=50, chunk_mode=None, pe='regular', skip_embedding=True)\n",
    "# print(model)\n",
    "print(model(torch.rand(32,1000,1)).size())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5533f865-aa90-4e89-a402-3e195ab64df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenericModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        self.net = LSTM(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "        # BiGRU(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "        # ConvGru(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "        # FullyConv(input_dim=1, hidden_dim=64, output_dim=1)\n",
    "        # FFN(input_dim=1, hidden_dim=64, output_dim=1, num_layers=3)\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net.forward(x)\n",
    "        return out[:,-1,:]\n",
    "  \n",
    "      \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      # print(\"LEARNING RATE:\",self.learning_rate)\n",
    "      optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.01) #wd 0.01\n",
    "      return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "      x,y = train_batch\n",
    "      output = self(x)\n",
    "      loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "      self.log('train_loss', loss)\n",
    "      acc =self.acc(output, y.int())\n",
    "      self.log('train acc', acc)\n",
    "      return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      x,y = val_batch\n",
    "      output = self(x)\n",
    "      loss = F.binary_cross_entropy_with_logits(output, y)\n",
    "      self.log('valid_loss', loss)\n",
    "      acc = self.acc(output, y.int())\n",
    "      self.log('valid acc', acc)\n",
    "\n",
    "rnn = GenericModel(0.001)\n",
    "rnn(torch.swapaxes(torch.rand(32,1,1000),-1,-2)).size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RNAmodEnv]",
   "language": "python",
   "name": "conda-env-RNAmodEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
